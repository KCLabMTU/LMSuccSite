{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6646fae-d370-479d-b7ce-ef8336ac866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 19:17:16.862702: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-03 19:17:16.867251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-03 19:17:16.867264: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import keras functions\n",
    "# import tensorflow\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Lambda, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.backend import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98087fc-8d21-4bf6-bca1-f7e9e31e8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_for_embedding(fasta_file):\n",
    "    encodings = []\n",
    "    \n",
    "    # define universe of possible input values\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    \n",
    "    # define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        data = seq_record.seq\n",
    "        for char in data:\n",
    "            if char not in alphabet:\n",
    "                return\n",
    "        integer_encoded = [char_to_int[char] for char in data]\n",
    "        encodings.append(integer_encoded)\n",
    "    encodings = np.array(encodings)\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb62885c-3a4f-4850-9241-a0443a67bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences to integer encoding, for embedding\n",
    "test_positive_embedding = get_input_for_embedding('data/Big_testset/fasta/test_positive_sites.fasta')\n",
    "test_negative_embedding = get_input_for_embedding('data/Big_testset/fasta/test_negative_sites.fasta')\n",
    "train_positive_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/positive_sites.fasta')\n",
    "train_negative_embedding = get_input_for_embedding('data/DeepSuccinylSite/fasta/negative_sites.fasta')\n",
    "\n",
    "# create labels\n",
    "train_positive_labels = np.ones(train_positive_embedding.shape[0])\n",
    "train_negative_labels = np.zeros(train_negative_embedding.shape[0])\n",
    "test_positive_labels = np.ones(test_positive_embedding.shape[0])\n",
    "test_negative_labels = np.zeros(test_negative_embedding.shape[0])\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_train_full_embedding = np.vstack((train_positive_embedding,train_negative_embedding))\n",
    "X_test_embedding = np.vstack((test_positive_embedding,test_negative_embedding))\n",
    "y_train_full = np.concatenate((train_positive_labels, train_negative_labels), axis = 0)\n",
    "y_test = np.concatenate((test_positive_labels, test_negative_labels), axis = 0)\n",
    "\n",
    "train_positive_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/train_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "train_negative_pt5 = pd.read_csv(\"data/DeepSuccinylSite/features/full/train_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_positive_pt5 = pd.read_csv(\"data/Big_testset/ProtT5_features/test_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_negative_pt5 = pd.read_csv(\"data/Big_testset/ProtT5_features/test_negative_big_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_train_pt5_full = np.vstack((train_positive_pt5,train_negative_pt5))\n",
    "X_test_pt5 = np.vstack((test_positive_pt5,test_negative_pt5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317adf7e-43d4-4610-bf90-fad3e49cf903",
   "metadata": {},
   "source": [
    "### Load trained base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f847402-c5f8-419f-9de6-31916e924b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(model_names):\n",
    "    all_models = list()\n",
    "    for model in model_names:\n",
    "        filename = 'selected_models/'+ model + '.h5'\n",
    "        model = load_model(filename)\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "# meta learner\n",
    "def define_stacked_model(members):\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(16, activation='relu', name = 'ds_1')(merge)\n",
    "    hidden = Dense(4, activation='relu', name = 'ds_2')(hidden)\n",
    "    output = Dense(1, activation='sigmoid', name = 'ds_4')(hidden)\n",
    "    model = Model(inputs = ensemble_visible, outputs = output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit a model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    inputy_enc = to_categorical(inputy)\n",
    "    model.fit(X, inputy_enc, epochs=10, verbose=1)   \n",
    "\n",
    "# prediction\n",
    "def predict_stacked_model(model, inputX):\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    return model.predict(X, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "edf1bbb4-71d1-4ed0-90c5-e4353766a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "297/297 [==============================] - 3s 7ms/step - loss: 0.3264 - accuracy: 0.8651\n",
      "Epoch 2/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2587 - accuracy: 0.8949\n",
      "Epoch 3/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2430 - accuracy: 0.9015\n",
      "Epoch 4/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2425 - accuracy: 0.8994\n",
      "Epoch 5/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2321 - accuracy: 0.9085\n",
      "Epoch 6/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2316 - accuracy: 0.9054\n",
      "Epoch 7/7\n",
      "297/297 [==============================] - 2s 7ms/step - loss: 0.2348 - accuracy: 0.9042\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "ANN = load_model(\"selected_models/ANN.h5\")\n",
    "Embedding = load_model(\"selected_models/Embedding.h5\")\n",
    "\n",
    "# create truncated models\n",
    "ANN_new = Model(inputs=ANN.input, outputs=ANN.get_layer(index=4).output)\n",
    "\n",
    "Embedding_new = Model(inputs=Embedding.input, outputs=Embedding.get_layer(index=6).output)\n",
    "\n",
    "\n",
    "stacked_model = define_stacked_model([Embedding_new, ANN_new])\n",
    "\n",
    "# fit_stacked_model(stacked_model, x_test, y_test)\n",
    "st_history = stacked_model.fit([X_train_full_embedding, X_train_pt5_full], y_train_full, epochs=7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40f1dc98-c23b-4f7d-92ba-de365b4c694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 4ms/step\n",
      "\n",
      " 0.7885926844389337, 0.34839809143939426, 0.7747035573122529, 0.789774638412378, [[2348  625]\n",
      " [  57  196]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performance evaluation\n",
    "y_pred_1 = stacked_model.predict([X_test_embedding, X_test_pt5])\n",
    "y_pred = (y_pred_1 > 0.5)\n",
    "y_pred = [int(i) for i in y_pred]\n",
    "y_test = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "sn = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "sp = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "\n",
    "# plot(history)\n",
    "print(\"\\n %s, %s, %s, %s, %s \\n\" %(str(acc), str(mcc), str(sn), str(sp), cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "925118c8-bb62-4a46-99d3-35ebcfd3c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked_model.save(\"selected_models/combined_feature_level_362_.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e79bd7-abe8-44ab-8593-12b52f24a904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
